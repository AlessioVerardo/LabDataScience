{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7794d6f9-9ca5-447f-aae8-df2caa39e007",
   "metadata": {},
   "source": [
    "# Basis graph construction\n",
    "\n",
    "In this notebook, we build the graph that will be the basis of our journey planner algorithm.\n",
    "\n",
    "## Setup PySpark\n",
    "We first start by loading the Spark session. Note that some of our queries are heavy and this notebook should be run only once, we allow ourselves take a bit more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6388e134-f5f6-4fa4-858e-c6bdb95bf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84155f4a-b5f4-4a19-a937-b8c3a682876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verardo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4047b54d-166b-49d8-9271-3995d0a9ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"http://iccluster029.iccluster.epfl.ch:8998\"\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_cell_magic('spark', line=\"config\",\n",
    "                             cell=\"\"\"{{ \"name\":\"{0}-aces\", \"executorMemory\":\"10G\", \"executorCores\":8, \"numExecutors\":10 }}\"\"\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a18bf1-c712-4cd5-b751-559af41e8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>9439</td><td>application_1652960972356_5268</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster029.iccluster.epfl.ch:8088/proxy/application_1652960972356_5268/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster056.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1652960972356_5268_01_000001/eric\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic(\n",
    "    \"spark\", \"add -s {0}-aces -l python -u {1} -k\".format(username, server) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917253eb-c34e-4f66-b25f-3e5f90f412e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515230be-484b-4cda-8d3f-a8dcc770687e",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Filter on service that are available every week day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced64eba-98cf-463d-a5af-2a041d179073",
   "metadata": {},
   "source": [
    "We choose the week of May 8th to build our model of the public infrastructure as this week do not have contain any of the bank holiday days of Switzerland. Since our planner will provide information only for week days (considering each week day as the same), we start by filtering out services that do not occur each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "063ab568-19ca-4145-9ccc-8fa95df9ff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "calendar = spark.read.csv('/data/sbb/csv/calendar/2019/05/08/calendar.txt', sep=',', header=True, inferSchema=True)\n",
    "\n",
    "# Rename all the columns by taking lower cases\n",
    "old_columns = calendar.schema.names\n",
    "new_columns = [col.lower() for col in old_columns]\n",
    "\n",
    "calendar = reduce(lambda data, idx: calendar.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8182d1-0d66-47c6-8c39-7e0ea57a03be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Keep only week days\n",
    "calendar_week = calendar.filter((calendar.monday == 1)\n",
    "                & (calendar.tuesday  == 1)\n",
    "                & (calendar.wednesday == 1)\n",
    "                & (calendar.thursday == 1)\n",
    "                & (calendar.friday == 1)).select('service_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227c1a3-8ee5-4cbb-8c37-813493833c35",
   "metadata": {},
   "source": [
    "Using the results, we filter out trips that do not occur in each week day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1d36c2b-0eb4-4743-8da0-fffd12257b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "trips = spark.read.csv('/data/sbb/csv/trips/2019/05/08/trips.txt', sep=',', header=True, inferSchema=True)\n",
    "\n",
    "# Rename all the columns by taking lower cases\n",
    "old_columns = trips.schema.names\n",
    "new_columns = [col.lower() for col in old_columns]\n",
    "\n",
    "trips = reduce(lambda data, idx: trips.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3803d8b5-281b-4502-8ecb-0d9800aad0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Join with the trip dataframe, to get the list of trips that happen only on all week days\n",
    "trip_id = calendar_week.join(trips, on='service_id', how='inner').select('trip_id', 'route_id').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75062ea7-4ab3-4c57-914c-07635a449865",
   "metadata": {},
   "source": [
    "### Add the stop information\n",
    "We load stop informations from HDFS and merge them with the trip information already loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce69185b-50db-4bcc-bcb4-4fb780a78fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "stop_times = spark.read.csv('/data/sbb/csv/stop_times/2019/05/08/stop_times.txt', sep=',', header=True, inferSchema=True)\n",
    "\n",
    "# Rename all the columns by taking lower cases\n",
    "old_columns = stop_times.schema.names\n",
    "new_columns = [col.lower() for col in old_columns]\n",
    "\n",
    "stop_times = reduce(lambda data, idx: stop_times.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), stop_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060095f6-9d26-44ce-868f-4ece34a7614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Get all the nodes \n",
    "nodes = trip_id.join(stop_times, on='trip_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9e731-6f74-4efd-8eef-ae143ef8ed2d",
   "metadata": {},
   "source": [
    "Finally, we include all stop detailled informations and we merge them in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51e3016-fb6d-434e-9290-4a080fddca42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "# Note that we need to specify the the format only for this file as PySpark cannot infer it.\n",
    "schema = StructType([StructField(\"stop_id\", StringType(), True),\n",
    "                    StructField(\"stop_name\", StringType(), True),\n",
    "                     StructField(\"stop_lat\", FloatType(), True),\n",
    "                    StructField(\"stop_lon\", FloatType(), True),\n",
    "                    StructField(\"location_type\", IntegerType(), True),\n",
    "                    StructField(\"parent_location\", StringType(), True)])\n",
    "stops = spark.read.csv('/data/sbb/csv/allstops/stop_locations.csv', sep=',', header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc26803-0fb1-4d7e-abe0-f636bef48ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "node_info = stops.join(nodes, on=\"stop_id\",how=\"inner\").cache() # Cache the results so that we do not need to compute this series of merge again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d29a6-0738-42c3-9b7a-2c1e9873f746",
   "metadata": {},
   "source": [
    "### Decoupling of arrival and departure nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652de07-fb18-4f88-8f9b-934ab9cd2c84",
   "metadata": {},
   "source": [
    "We perform some processing on the nodes: \n",
    "- We add one new column `IS_ARRIVAL` whose purpose is simply to tell us if the node is the arrival or the departure of the current 'edge' \n",
    "- We start by keeping only the arrival time of the node and add a column `IS_ARRIVAL` full of one. \n",
    "- Then, we do the same with the daprture time and we fill the `IS_ARRIVAL` column with zeos. \n",
    "- Take the union of the two processings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb04f38-a8f5-45aa-bb24-e62a57fae27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import *\n",
    "# Duplicate the nodes so that we have one arrival node and one departure node for each of the stop times\n",
    "all_nodes_arr = node_info.drop(node_info.departure_time).withColumnRenamed(\"arrival_time\",\"time\")\n",
    "all_nodes_arr = all_nodes_arr.withColumn(\"is_arrival\",lit(1))\n",
    "all_nodes_dep = node_info.drop(node_info.arrival_time).withColumnRenamed(\"departure_time\",\"time\")\n",
    "all_nodes_dep = all_nodes_dep.withColumn(\"is_arrival\", lit(0))\n",
    "all_nodes = all_nodes_arr.union(all_nodes_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6e053-76df-42d3-8d2a-081868eb8776",
   "metadata": {},
   "source": [
    "### Keep only nodes around Zürich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17586d-813d-4273-bfa4-776191ddf6c9",
   "metadata": {},
   "source": [
    "We define the [haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) function which will help us to compute the distance between two coordinates (in meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658dee58-1355-43e3-a6bf-395ca83a0d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "from math import radians, cos, sin, asin, sqrt, atan2\n",
    "\n",
    "@F.udf\n",
    "def haversine(lat1, lon1, lat2=47.378177, lon2=8.540192):\n",
    "    \"\"\"\n",
    "    Compute the haversine distance between two coordinates\n",
    "    :param lat1: the latitude of the first point\n",
    "    :param lon1: the longitude of teh first point\n",
    "    :param lat2: the latitude of the second point, by default the one of Zurich HB\n",
    "    :param lon2: the longitude of the second point, by default the one of Zurich HB\n",
    "    \"\"\"\n",
    "    # https://www.movable-type.co.uk/scripts/latlong.html\n",
    "    # Zurich HB coordinates by default\n",
    "    \n",
    "    R = 6371e3\n",
    "    phi1, phi2, delta_phi, delta_lambda = map(radians, [lat1, lat2, lat2 - lat1, lon2 - lon1])\n",
    "    a = sin(0.5 * delta_phi)**2 + cos(phi1) * cos(phi2) * sin(0.5 * delta_lambda)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return c * R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588937da-b999-4237-ae4f-4c2ba251d9df",
   "metadata": {},
   "source": [
    "Using the previously defined function, we can now filter out every node that is further away than 15km from Zurich HB. To be safe, we include 2.5km of margin. This supplementary margin is a trade-off between being able to reach more nodes and not to have too many walking edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d42714-7d9c-414a-aa17-ed78a35be4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Filter out data that are too far\n",
    "MARGIN = 2.5e3\n",
    "RADIUS = 15e3\n",
    "\n",
    "# Filter to keep nodes only during business hours\n",
    "nodes_zurich = (all_nodes.filter(haversine(all_nodes.stop_lat, all_nodes.stop_lon) < RADIUS + MARGIN)\n",
    "                .filter((hour(to_timestamp(col('time'), format=\"HH:mm:ss\")) >= 6)\n",
    "                                                 & (hour(to_timestamp(col('time'), format=\"HH:mm:ss\")) <= 20))\n",
    "                .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf3c415-6d54-4038-8058-2c773930f7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+-------------+---------------+--------------------+-----------+--------+-------------+-----------+-------------+----------+\n",
      "|    stop_id| stop_name| stop_lat|stop_lon|location_type|parent_location|             trip_id|   route_id|    time|stop_sequence|pickup_type|drop_off_type|is_arrival|\n",
      "+-----------+----------+---------+--------+-------------+---------------+--------------------+-----------+--------+-------------+-----------+-------------+----------+\n",
      "|    8503064|  Scheuren|47.322598|8.659553|         null|           null|1.TA.26-18-j19-1.1.H|26-18-j19-1|10:41:00|            1|          0|            0|         1|\n",
      "|8503065:0:1|     Forch|47.325336|8.647973|         null|  Parent8503065|1.TA.26-18-j19-1.1.H|26-18-j19-1|10:45:00|            2|          0|            0|         1|\n",
      "|    8503074|Neue Forch|47.325813| 8.63784|         null|           null|1.TA.26-18-j19-1.1.H|26-18-j19-1|10:46:00|            3|          0|            0|         1|\n",
      "|    8503068|  Maiacher|47.328167|8.631067|         null|           null|1.TA.26-18-j19-1.1.H|26-18-j19-1|10:47:00|            4|          0|            0|         1|\n",
      "|    8503066|   Zumikon|47.331688|8.623809|         null|           null|1.TA.26-18-j19-1.1.H|26-18-j19-1|10:48:00|            5|          0|            0|         1|\n",
      "+-----------+----------+---------+--------+-------------+---------------+--------------------+-----------+--------+-------------+-----------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(None, 697279)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "nodes_zurich.show(5), nodes_zurich.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339226a-bc63-485f-833f-2ca95df3a1b6",
   "metadata": {},
   "source": [
    "We can see that some of the stop ids contain \"$:0:x$\" as described in the data cookbook, where $x$ is the platform number. This corresponds to stops belonging to a bigger place, for instance Zurich mainstation has multiple platforms. They will all start with the id of Zurich mainstation but then contains \"$:0:x$\" where $x$ is the track number. From the cookbook, we can see that the '0' could be in fact replaced by other number, let see if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b7d3cec-2273-4412-afd6-8456e14a1851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50151, 50151)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "nodes_zurich.filter(nodes_zurich.stop_id.contains(\":0:\")).count(), nodes_zurich.filter(nodes_zurich.stop_id.contains(\":\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641e5c5-b0fa-486c-95c8-d722f86f4982",
   "metadata": {},
   "source": [
    " We can see that there is no node whose id contains \"_:y:x_\" where $y\\neq 0$. Thus, this doesn't contain any information for our purposes and we can safely remove :0 from the node that contains this. \n",
    " \n",
    " We also provide a way of giving an unique_id to each of the node to be able to identify them in the graph. This unique identifier has the following shape:\n",
    "$$\n",
    "STOPID||\\_||TIME||\\_||TRIPID||\\_||ISARRIVAL\n",
    "$$\n",
    "where ∣∣ denotes the concatenation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e114b9fe-0c10-4178-acac-9f47644c40d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697279"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Use the concat_ws to concatenate all the given colun with '_' separator\n",
    "nodes_zurich = nodes_zurich.withColumn(\"full_stop_id\",\n",
    "                                       F.concat_ws(\"_\", F.regexp_replace(\"stop_id\",\":0\",\"\"),\n",
    "                                                F.regexp_replace(\"time\",\":\",\"-\"),\n",
    "                                                nodes_zurich.trip_id,\n",
    "                                                nodes_zurich.is_arrival)).cache()\n",
    "nodes_zurich.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e843f08-a010-4103-b5c5-9bcf11d22b14",
   "metadata": {},
   "source": [
    "We can see that the number of nodes is odd which can seem strange since we duplicate every stop/start node. However, if a bus arrives at 20:59 at one station and continue its trip at 21:01, then we can see that the second one will be dropped whereas the first one will be kept hence the odd number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4a17-0890-4be5-a7f0-9bac3471fafc",
   "metadata": {},
   "source": [
    "Since we now have our definite list of nodes, we can save it on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43beb6da-1920-4470-a621-b4045571f31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "nodes_zurich.write.save(\"/group/aces/graph/nodes_final.orc\", format=\"orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9391d8-df4a-4dcc-baeb-a5cdcffdaaf8",
   "metadata": {},
   "source": [
    "## Build edges\n",
    "In this part, we will build all the different edges we want to consider in our graph reprsentation of the Zurich public transport model. The edges we consider are \n",
    "1. Intra-station edge. I.e. for nodes that have the same non-null parent station attribute and correspond to trip that are no more than 15 minutes apart, we add an edge with 2 min of walking times as indicated in the statement.\n",
    "2. We add walking edges between stops that are no more than 15 minutes apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437a8e0-75be-496b-b408-3e9097f26b1a",
   "metadata": {},
   "source": [
    "### Add the timetable information to the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0c33fe-c296-4a12-a10d-3d50139a1f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "id_nodes_zh = nodes_zurich.select(\"stop_id\", \"trip_id\",\"route_id\",\"full_stop_id\",\"time\").distinct()\n",
    "\n",
    "# Create the dataframes containing all zurich nodes and with all the information\n",
    "stop_times_zh_arr = (stop_times.join(\n",
    "    id_nodes_zh.filter(col(\"is_arrival\")==1).select(\n",
    "        col(\"stop_id\"),\n",
    "        col(\"trip_id\"),\n",
    "        col(\"route_id\"),\n",
    "        col(\"full_stop_id\"),\n",
    "        col(\"time\").alias(\"arrival_time\")\n",
    "), on=[\"stop_id\", \"trip_id\",\"arrival_time\"], how=\"inner\"))\n",
    "stop_times_zh_dep = (stop_times.join(\n",
    "    id_nodes_zh.filter(col(\"is_arrival\")==0).select(\n",
    "        col(\"stop_id\"),\n",
    "        col(\"trip_id\"),\n",
    "        col(\"route_id\"),\n",
    "        col(\"full_stop_id\"),\n",
    "        col(\"time\").alias(\"departure_time\")\n",
    "), on=[\"stop_id\", \"trip_id\",\"departure_time\"], how=\"inner\"))\n",
    "\n",
    "stop_times_zh = stop_times_zh_dep.union(stop_times_zh_arr).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d08c8-31a0-4087-9857-5573773b2c01",
   "metadata": {},
   "source": [
    "### Load the routes information\n",
    "We can now load the road information such as the road number, the transport mean used and the road id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe2acb3-97e5-412c-a3e4-4f854d865b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "routes = spark.read.csv('/data/sbb/csv/routes/2019/05/08/routes.txt', sep=',', header=True, inferSchema=True)\n",
    "\n",
    "# Rename all the columns in the route dataframe to lower cases\n",
    "old_columns = routes.schema.names\n",
    "new_columns = [col.lower() for col in old_columns]\n",
    "\n",
    "routes = reduce(lambda data, idx: routes.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad374ea-553d-49a4-866c-719352c1bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(route_desc=u'TGV'), Row(route_desc=u'Eurocity'), Row(route_desc=u'Standseilbahn'), Row(route_desc=u'Regionalzug'), Row(route_desc=u'RegioExpress'), Row(route_desc=u'S-Bahn'), Row(route_desc=u'Luftseilbahn'), Row(route_desc=u'Sesselbahn'), Row(route_desc=u'Taxi'), Row(route_desc=u'F\\xe4hre'), Row(route_desc=u'Tram'), Row(route_desc=u'ICE'), Row(route_desc=u'Bus'), Row(route_desc=u'Gondelbahn'), Row(route_desc=u'Nacht-Zug'), Row(route_desc=u'Auoreisezug'), Row(route_desc=u'Eurostar'), Row(route_desc=u'Schiff'), Row(route_desc=u'Schnellzug'), Row(route_desc=u'Intercity'), Row(route_desc=u'InterRegio'), Row(route_desc=u'Extrazug'), Row(route_desc=u'Metro')]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "routes.select('route_desc').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8bcb5-4afe-448a-a501-3239f30968ab",
   "metadata": {},
   "source": [
    "We can see that there is a lot of different transport means but they often have a common base, i.e. InterRegio, Intercity, ExtraZug, ... all refers to train. So we first map them into a common bases which will be one of the following: _Train, Tram, Bus or other_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c283bfb-197a-4059-8314-a2793e9246dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "transport_mapping = {\n",
    "    \"Auoreisezug\": \"Other\",\n",
    "    \"Bus\": \"Bus\",\n",
    "    \"Eurostar\": \"Train\",\n",
    "    \"Eurocity\": \"Train\",\n",
    "    \"Extrazug\": \"Train\",\n",
    "    u\"F\\xe4hre\": \"Other\",\n",
    "    \"Gondelbahn\": \"Other\",\n",
    "    \"ICE\": \"Train\",\n",
    "    \"Intercity\": \"Train\",\n",
    "    \"InterRegio\": \"Train\",\n",
    "    \"Luftseilbahn\": \"Other\",\n",
    "    \"Metro\": \"Other\",\n",
    "    \"Nacht-Zug\": \"Train\",\n",
    "    \"RegioExpress\": \"Train\",\n",
    "    \"Regionalzug\": \"Train\",\n",
    "    \"S-Bahn\": \"Train\",\n",
    "    \"Schiff\": \"Other\",\n",
    "    \"Schnellzug\": \"Train\",\n",
    "    \"Sesselbahn\": \"Other\",\n",
    "    \"Standseilbahn\": \"Other\", \n",
    "    \"Taxi\": \"Other\",\n",
    "    \"TGV\": \"Train\",\n",
    "    \"Tram\": \"Tram\"\n",
    "}\n",
    "\n",
    "@F.udf\n",
    "def map_transport(x):\n",
    "    return transport_mapping[x]\n",
    "\n",
    "routes = routes.withColumn('route_desc', map_transport(routes.route_desc)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20616584-9a64-4fbe-b37a-9a50f9725d94",
   "metadata": {},
   "source": [
    "### Merge with nodes\\_zurich\n",
    "We can now merge all the information into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1fea555-6dc3-4ee3-834d-df26980fd0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697279"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "nodes_zh_all_infos = nodes_zurich.join(routes, on=\"route_id\").select(\"full_stop_id\",\"stop_name\",\"stop_lat\",\"stop_lon\", \"time\", \"route_short_name\",\"route_desc\")\n",
    "nodes_zh_all_infos.write.save(\"/group/aces/graph/nodes_all_info_final.orc\", format=\"orc\", mode='overwrite')\n",
    "nodes_zh_all_infos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3061d6-abf1-473d-8a90-e5d7846f907a",
   "metadata": {},
   "source": [
    "### Build the transport edges\n",
    "\n",
    "In this section, we will build the different transport edges. Each edge will link a departure node of one stop to the arrival node of the subsequent stop in the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e169ecea-b16c-45da-b891-2bde104482b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "# We lag the stop_id, arrival_time_dest and full_stop_id_dest by 1 upward so that we obtain pairs of subsequent stops\n",
    "stop_times_zh_pairs = stop_times_zh.withColumn('stop_id_dest', F.lag('stop_id', count=-1).over(Window.partitionBy('trip_id').orderBy([col('stop_sequence').asc(), col(\"departure_time\").asc(),col('full_stop_id').desc()])))\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.withColumn('arrival_time_dest', F.lag('arrival_time', count=-1).over(Window.partitionBy('trip_id').orderBy([col('stop_sequence').asc(), col(\"departure_time\").asc(),col('full_stop_id').desc()])))\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.withColumn('full_stop_id_dest', F.lag('full_stop_id', count=-1).over(Window.partitionBy('trip_id').orderBy([col('stop_sequence').asc(), col(\"departure_time\").asc(),col('full_stop_id').desc()])))\n",
    "# We drop the arrival_time column and we change the arrival_time_dest to the arrival_time column\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.drop('arrival_time').withColumnRenamed('arrival_time_dest', 'arrival_time')\n",
    "# Drop the destination stop id as we already have the full_stop id containing all teh information\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.dropna(subset='stop_id_dest')\n",
    "# Compute the expected travel time, i.e. the time between the arrival_time at the finish stop and the departure time at the departure node\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.withColumn('expected_travel_time', to_timestamp(stop_times_zh_pairs.arrival_time, 'HH:mm:ss').cast('long') - to_timestamp(stop_times_zh_pairs.departure_time, 'HH:mm:ss').cast('long'))\n",
    "\n",
    "# Drop edges that are from and to same stop id (is_arrival : one for departure and one for arrival with same node id)\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.filter(stop_times_zh_pairs.stop_id != stop_times_zh_pairs.stop_id_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e51bde-0868-466d-b557-c118651ee52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Add the routes information\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.join(routes.select('route_id', 'route_desc', 'route_short_name'), on='route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a42fb6f8-9595-4093-b4f6-c0bec8817191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Keep transports only during working hours\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.filter((hour(to_timestamp(col('departure_time'), format=\"HH:mm:ss\")) >= 6)\n",
    "                                                 & (hour(to_timestamp(col('arrival_time'), format=\"HH:mm:ss\")) <= 20))\n",
    "\n",
    "# Select only necessary columns\n",
    "stop_times_zh_pairs = stop_times_zh_pairs.select(\n",
    "    col(\"full_stop_id\").alias(\"start_id\"),\n",
    "    col(\"full_stop_id_dest\").alias(\"end_id\"),\n",
    "    col(\"expected_travel_time\").alias(\"duration\"),\n",
    "    col(\"route_desc\").alias(\"transport\"),\n",
    "    col('route_short_name').alias(\"line_number\"),\n",
    "    F.lit(1).alias(\"is_trip\"),\n",
    "    F.lit(0).alias(\"waiting_time\"),\n",
    "    hour(to_timestamp(stop_times_zh_pairs.arrival_time, 'HH:mm:ss')).alias(\"hour\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4d256-20e2-4245-ae8b-ee6a0420e8f8",
   "metadata": {},
   "source": [
    "The format of the edge table is the following:\n",
    "- `start_id`: the full stop id of the starting node\n",
    "- `end_id` : the full stop id of the ending node\n",
    "- `mean_delay`: 1/the parameters of the exponential to take into account when computing the probability of success of this edge (0 for the walking edges).\n",
    "- `std_delay`: the standard deviation of the delay of the transports between the two stops (1.0 for the walking edges).\n",
    "- `median_delay`:the median of the delay of the transports between the two stops (0 for the walking edges).\n",
    "- `duration`: is the expected travel time for transport edge or the walking time for walking edges.\n",
    "- `transport`: the type of transport used or feet if it corresponds to walking edge.\n",
    "- `line_number`: transport line number or -1 if walking edge.\n",
    "- `is_trip`: boolean with value 1 if transport edge and 0 if walking edge.\n",
    "- `waiting_time`:-1 if transport edge or the waiting time if this is a walking edge.\n",
    "- `hour`: the hour at which the transport is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d008352-3acb-4735-900a-cee02b8916b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------------------------+--------+---------+-----------+-------+------------+----+\n",
      "|start_id                                 |end_id                                   |duration|transport|line_number|is_trip|waiting_time|hour|\n",
      "+-----------------------------------------+-----------------------------------------+--------+---------+-----------+-------+------------+----+\n",
      "|8503064_10-41-00_1.TA.26-18-j19-1.1.H_0  |8503065:1_10-45-00_1.TA.26-18-j19-1.1.H_1|240     |Train    |18         |1      |0           |10  |\n",
      "|8503065:1_10-45-00_1.TA.26-18-j19-1.1.H_0|8503074_10-46-00_1.TA.26-18-j19-1.1.H_1  |60      |Train    |18         |1      |0           |10  |\n",
      "|8503074_10-46-00_1.TA.26-18-j19-1.1.H_0  |8503068_10-47-00_1.TA.26-18-j19-1.1.H_1  |60      |Train    |18         |1      |0           |10  |\n",
      "|8503068_10-47-00_1.TA.26-18-j19-1.1.H_0  |8503066_10-48-00_1.TA.26-18-j19-1.1.H_1  |60      |Train    |18         |1      |0           |10  |\n",
      "|8503066_10-48-00_1.TA.26-18-j19-1.1.H_0  |8503075_10-50-00_1.TA.26-18-j19-1.1.H_1  |120     |Train    |18         |1      |0           |10  |\n",
      "+-----------------------------------------+-----------------------------------------+--------+---------+-----------+-------+------------+----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "stop_times_zh_pairs.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea27066c-19a8-45b3-9a36-d130cca74688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "stop_times_zh_pairs.write.save(\"/group/aces/graph/edges_transport_final.orc\", format=\"orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc353195-cf48-4bbb-8ccb-8537b4822e79",
   "metadata": {},
   "source": [
    "### Intra-station edges\n",
    "Here we add the edge needed to link arrival nodes and departure nodes within a same station. We add only the following edges:\n",
    "- Pairs of nodes that have non null and same parent_location (i.e. in same station) but different platforms (different stop_id), with a default 2 minutes walking time.\n",
    "- Pairs of nodes that have same stop_id, with a walking time of 0. This contains 2 types of edges :\n",
    "    - Edges in same (non-null) parent station and same platforms (same stop_id), i.e. does not get off the transport.\n",
    "    - Edges with null parent station but same stop_id (e.g. a bus stop), i.e. does not get off the transport.\n",
    "    \n",
    "We made the assumption that the maximal acceptable waiting for someone is 15 minutes. Therefore, we remove any edge which doesn't match this constraint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b58a9db4-6b86-4116-b77f-ae08a1a5cc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "acceptable_waiting_time = 15*60\n",
    "walking_time_in_station = 2*60\n",
    "\n",
    "# keep only the useful attributes from the nodes in the Zurich area\n",
    "nodes_zurich_tmp = nodes_zurich.select(\n",
    "    nodes_zurich.is_arrival,\n",
    "    nodes_zurich.full_stop_id,\n",
    "    to_timestamp(nodes_zurich.time, 'HH:mm:ss').cast('long').alias('time'),\n",
    "    nodes_zurich.parent_location,\n",
    "    nodes_zurich.stop_id\n",
    ")\n",
    "\n",
    "# Need to call directly after the cross join to be able to optimize it\n",
    "# Here we select the nodes belonging to the same station but that are on different platforms.\n",
    "all_same_station_edges_cross_diff_platforms = (\n",
    "                            # Select only arrival node\n",
    "                            nodes_zurich_tmp\n",
    "                            .filter(nodes_zurich_tmp.is_arrival == 1)\n",
    "                            .select(nodes_zurich_tmp.full_stop_id.alias(\"start_id\"),\n",
    "                                      nodes_zurich_tmp.time.alias(\"arr_time\"),\n",
    "                                      nodes_zurich_tmp.parent_location.alias(\"arr_parent\"),\n",
    "                                      nodes_zurich_tmp.stop_id.alias(\"start_stop_id\"))\n",
    "                            .crossJoin(\n",
    "                                # Select only departure node \n",
    "                                nodes_zurich_tmp\n",
    "                                        .filter(nodes_zurich_tmp.is_arrival == 0)\n",
    "                                        .select(\n",
    "                                            nodes_zurich_tmp.full_stop_id.alias(\"end_id\"),\n",
    "                                            nodes_zurich_tmp.time.alias(\"dep_time\"),\n",
    "                                            nodes_zurich_tmp.parent_location.alias(\"dep_parent\"),\n",
    "                                            nodes_zurich_tmp.stop_id.alias(\"end_stop_id\"))\n",
    "                            ).filter(\n",
    "                                # Filter on pair of nodes whose both stops parent id are not null and are the same \n",
    "                                col(\"dep_parent\").isNotNull() & col(\"arr_parent\").isNotNull() & (col(\"arr_parent\") == col(\"dep_parent\")) & (col('start_stop_id') != col('end_stop_id'))\n",
    "                            ))\n",
    "\n",
    "# Here, we will build the edges for the same stop and subsequent trips. \n",
    "# This dataframe will create an edge between all the transport arriviing at each step\n",
    "all_same_station_edges_cross_same_stop = (nodes_zurich_tmp\n",
    "                            # Select only arrival node\n",
    "                            .filter(nodes_zurich_tmp.is_arrival == 1)\n",
    "                            .select(nodes_zurich_tmp.full_stop_id.alias(\"start_id\"),\n",
    "                                      nodes_zurich_tmp.time.alias(\"arr_time\"),\n",
    "                                    nodes_zurich_tmp.stop_id.alias(\"start_stop_id\"))\n",
    "                            .crossJoin(\n",
    "                                # Select only departure node\n",
    "                                nodes_zurich_tmp\n",
    "                                        .filter(nodes_zurich_tmp.is_arrival == 0)\n",
    "                                        .select(\n",
    "                                            nodes_zurich_tmp.full_stop_id.alias(\"end_id\"),\n",
    "                                            nodes_zurich_tmp.time.alias(\"dep_time\"),\n",
    "                                            nodes_zurich_tmp.stop_id.alias(\"end_stop_id\"))\n",
    "                            ).filter(\n",
    "                                # Keep the edges with the same stopID, i.e. in the same station\n",
    "                                col('start_stop_id') == col('end_stop_id')\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bed4fcc-d874-4f04-ad83-d162f961ccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Keep nodes with same parent stations but different platforms\n",
    "all_same_station_edges_diff_platforms = (all_same_station_edges_cross_diff_platforms\n",
    "                             # Default walking time between platforms\n",
    "                            .withColumn(\"duration\", F.lit(walking_time_in_station))\n",
    "                            .withColumn(\"waiting_time\", col(\"dep_time\")-col(\"arr_time\")-col(\"duration\"))\n",
    "                            .filter((col(\"waiting_time\") < acceptable_waiting_time) & (col(\"waiting_time\") >= 0)) #Filter out the edges which don't match the time constraint of the roblem\n",
    "                            .select(col(\"start_id\"),\n",
    "                                      col(\"end_id\"),\n",
    "                                      F.lit(0).alias(\"mean_delay\"), # Default value for the mean (since the time on walking/waiting edge is deterministic)\n",
    "                                      F.lit(1.0).alias(\"std_delay\"), # Default value for the std (since the time on walking/waiting edge is deterministic)\n",
    "                                      F.lit(0).alias(\"median_delay\"), # Default value for the median (since the time on walking/waiting edge is deterministic)\n",
    "                                      col('duration'), \n",
    "                                      F.lit(\"feet\").alias(\"transport\"),\n",
    "                                      F.lit(\"-1\").alias(\"line_number\"), # Default value for the line number when we are not in a line (useful for probability computation)\n",
    "                                      F.lit(0).alias(\"is_trip\"), # is_trip is 1 only for transport edges\n",
    "                                      col(\"waiting_time\"),\n",
    "                                      F.lit(-1).alias(\"hour\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3981135-cf10-4199-9a02-b60a4beceaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Need to filter the stops missing in the stop_times files\n",
    "line_numbers_zh = stop_times_zh_pairs.select('line_number').rdd.map(lambda x: x.line_number).distinct().collect()\n",
    "trip_id2line_nb = routes.join(trips, on='route_id', how='inner').select('trip_id', 'route_short_name').distinct().filter(col('route_short_name').isin(line_numbers_zh)).collect()\n",
    "# We build a dictionnary mapping the trip id to the route short name (the line number)\n",
    "trip_id2line_nb = {r['trip_id']: r['route_short_name'] for r in trip_id2line_nb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84b9715d-0f1e-4466-81fd-3d68f649e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "@F.udf\n",
    "def map_line_nb(arr_id, end_id):\n",
    "    \"\"\"\n",
    "    Give a line number to the intra station walking edges\n",
    "    if the edge is in the continuity of a trip. This is needed\n",
    "    to correctly compute the confidence of a given path.\n",
    "    :return: -1 if the two given id doesn't belong to the same stop, \n",
    "             -2 if the trip is not valid\n",
    "             the line number otherwise\n",
    "    \"\"\"\n",
    "    arr_stop_id = arr_id.split('_')[0]\n",
    "    arr_trip_id = arr_id.split('_')[2]\n",
    "    \n",
    "    end_stop_id = end_id.split('_')[0]\n",
    "    end_trip_id = end_id.split('_')[2]\n",
    "    \n",
    "    if (arr_stop_id == end_stop_id) and (arr_trip_id == end_trip_id):\n",
    "        if arr_trip_id in trip_id2line_nb:\n",
    "            return trip_id2line_nb[arr_trip_id]\n",
    "        else:\n",
    "            # If the trip id is not in the dictionnary, we consider the trip as invalid\n",
    "            return \"-2\"\n",
    "    else:\n",
    "        return \"-1\"\n",
    "\n",
    "# Keep nodes having same stop id, i.e. does change the platform \n",
    "all_same_station_edges_same_platform = (all_same_station_edges_cross_same_stop                                        \n",
    "                            # 0 walking time is 0 since does not change the plateform\n",
    "                            .withColumn(\"duration\", F.lit(0))\n",
    "                            .withColumn(\"waiting_time\", col(\"dep_time\")-col(\"arr_time\")-col(\"duration\"))\n",
    "                            .filter((col(\"waiting_time\") < acceptable_waiting_time) & (col(\"waiting_time\") >= 0)) # Filer edges whose waiting time are above the maximal waiting time\n",
    "                            .select(col(\"start_id\"),\n",
    "                                      col(\"end_id\"),\n",
    "                                      F.lit(0).alias(\"mean_delay\"), # Default value for the mean (since the time on walking/waiting edge is deterministic)\n",
    "                                      F.lit(1.0).alias(\"std_delay\"), # Default value for the std (since the time on walking/waiting edge is deterministic)\n",
    "                                      F.lit(0).alias(\"median_delay\"), # Default value for the median (since the time on walking/waiting edge is deterministic)\n",
    "                                      col('duration'),\n",
    "                                      F.lit(\"feet\").alias(\"transport\"),\n",
    "                                      F.lit(\"-1\").alias(\"line_number\"), # Default value for the line number when we are not in a line (useful for probability computation)\n",
    "                                      F.lit(0).alias(\"is_trip\"), # is_trip is 1 only for transport edges\n",
    "                                      col(\"waiting_time\"),\n",
    "                                      F.lit(-1).alias(\"hour\"))#Since the hour is used only to get the mean delay and there is no mean dealy in walking time, we can put -1\n",
    "                            )\n",
    "\n",
    "# remove all edges whose station is not on a valid trip_id (not in the stop_times file)\n",
    "all_same_station_edges_same_platform = all_same_station_edges_same_platform.withColumn('line_number', map_line_nb(col('start_id'), col('end_id')))\n",
    "all_same_station_edges_same_platform = all_same_station_edges_same_platform.filter(col('line_number') != '-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5db747f0-5e68-462a-aa94-ef94bc933040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Make the union of all the edges\n",
    "all_same_station_edges = all_same_station_edges_diff_platforms.union(all_same_station_edges_same_platform).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61ca1166-2ffb-40e3-8285-f3659cc95cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Save it to HDFS\n",
    "all_same_station_edges.write.save(\"/group/aces/graph/all_same_station_edges_final.orc\", format=\"orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cf9718d-237e-48d4-9d20-e43dd6999210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3260206"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "all_same_station_edges.count() # 3_260_206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b2c73a5-d9c4-4d64-bbd8-08e47a8ac8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697279"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "nodes_zurich.count() # 697_279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55204086-a868-4ea2-ad6f-3fe152a32eb1",
   "metadata": {},
   "source": [
    "### Build the walking edges\n",
    "Here we add the walking edges between close stations. The edges satisfying the following conditions are added :\n",
    "- End points are at most 500m appart (from the problem statement)\n",
    "- End points must :\n",
    "    - Either have both null parent station and different stop_id\n",
    "    - Or have different stop_id and different parent station (one potentially null)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bb52abb-6af7-4e5e-8b50-8a3a2a2ed311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "max_distance = 500\n",
    "meter_second = 60.0 / 50 # Need to divide float by integer, since the Scala interpret in the backend thinks that 60/50 = 1\n",
    "\n",
    "# Keep only the nodes that are on the specified perimter\n",
    "nodes_zh = node_info.filter(haversine(all_nodes.stop_lat, all_nodes.stop_lon) < RADIUS + MARGIN).cache()\n",
    "\n",
    "# Select all the pairs of nodes such that the ids are not the same and their distance is smaller than the max_distance\n",
    "all_pairs_stop_id = (nodes_zh.select(nodes_zh.stop_id.alias(\"start_id\"),\n",
    "                          nodes_zh.stop_lat.alias(\"arr_lat\"),\n",
    "                          nodes_zh.stop_lon.alias(\"arr_lon\"),\n",
    "                          nodes_zh.parent_location.alias(\"arr_parent\"))\n",
    "                 .distinct()\n",
    "                 .crossJoin(nodes_zh\n",
    "                            .select(\n",
    "                                nodes_zh.stop_id.alias(\"end_id\"),\n",
    "                                nodes_zh.stop_lat.alias(\"dep_lat\"),\n",
    "                                nodes_zh.stop_lon.alias(\"dep_lon\"),\n",
    "                                nodes_zh.parent_location.alias(\"dep_parent\"))\n",
    "                            .distinct()\n",
    "                   )\n",
    "                .filter(\n",
    "                    (col('start_id') != col('end_id')) & \n",
    "                    ((col('dep_parent').isNull() & col('arr_parent').isNull()) |\n",
    "                     (col('dep_parent').isNotNull() & col('arr_parent').isNull()) |\n",
    "                     (col('dep_parent').isNull() & col('arr_parent').isNotNull()) |\n",
    "                     (col('dep_parent') != col('arr_parent')))\n",
    "                )\n",
    "                .withColumn(\"distance\", haversine(col(\"arr_lat\"), col(\"arr_lon\"), col(\"dep_lat\"), col(\"dep_lon\"))) # Compute the distance between the two stops\n",
    "                .filter(col(\"distance\") <= max_distance) #Filter out the stops that are larger than the required distance.\n",
    "                .withColumn(\"walking_time\", col(\"distance\") * meter_second)) # Compute walking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a348ea5-bcc8-4217-a801-ad918b71a833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Finally, w create all the walking edges between the pairs of node id selected before.\n",
    "# We must be careful and select the arrival edge for the left hand of the edge and departure edge for the right end of the edge\n",
    "all_walking_edges = (all_pairs_stop_id.join(\n",
    "                    nodes_zurich.filter(nodes_zurich.is_arrival == 1)\n",
    "                                .select(\n",
    "                                        to_timestamp(nodes_zurich.time, 'HH:mm:ss').cast('long').alias(\"arr_time\"),\n",
    "                                        nodes_zurich.stop_id.alias(\"start_id\"),\n",
    "                                        col(\"time\").alias(\"arr_time_h\"),\n",
    "                                        nodes_zurich.full_stop_id.alias(\"full_arr_id\")), on=[\"start_id\"], how='inner')\n",
    "                .join(\n",
    "                    nodes_zurich.filter(nodes_zurich.is_arrival == 0)\n",
    "                                .select(\n",
    "                                        to_timestamp(nodes_zurich.time, 'HH:mm:ss').cast('long').alias(\"dep_time\"),\n",
    "                                        nodes_zurich.stop_id.alias(\"end_id\"),\n",
    "                                        col(\"time\").alias(\"dep_time_h\"),\n",
    "                                        nodes_zurich.full_stop_id.alias(\"full_dep_id\")), on=[\"end_id\"], how='inner')\n",
    "                # filter out all the pairs that doesn't match the time requiremeent\n",
    "                .filter(((col(\"dep_time\") - col(\"arr_time\")) >= col(\"walking_time\")) & ((col(\"dep_time\")-col(\"arr_time\")-col(\"walking_time\")) < acceptable_waiting_time))\n",
    "                # select the correct column in the correct order\n",
    "                .select(col(\"full_arr_id\").alias(\"start_id\"),\n",
    "                      col(\"full_dep_id\").alias(\"end_id\"),\n",
    "                      F.lit(0).alias(\"mean_delay\"), # Default value for the mean (since the time on walking/waiting edge is deterministic)\n",
    "                      F.lit(1.0).alias(\"std_delay\"), # Default value for the std (since the time on walking/waiting edge is deterministic)\n",
    "                      F.lit(0).alias(\"median_delay\"), # Default value for the median (since the time on walking/waiting edge is deterministic)\n",
    "                      col('walking_time').alias('duration'),\n",
    "                      F.lit(\"feet\").alias(\"transport\"),\n",
    "                      F.lit(\"-1\").alias(\"line_number\"), # Default value for the line number when we are not in a line (useful for probability computation)\n",
    "                      F.lit(0).alias(\"is_trip\"), # is_trip is 1 only for transport edges\n",
    "                      (col(\"dep_time\")-col(\"arr_time\")-col(\"walking_time\")).alias(\"waiting_time\"),\n",
    "                      F.lit(-1).alias('hour'))).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "377b7948-1b79-4b51-b300-2ae9e1e4869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "# Save on disk\n",
    "all_walking_edges.write.save(\"/group/aces/graph/all_walking_edges_final.orc\", format=\"orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b54f81e9-b37f-4f91-b990-751894e55940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12754187"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "all_walking_edges.count() # 12_754_187 with 500m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
